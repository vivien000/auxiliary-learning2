{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "experiment-1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWcjFyBzfM1R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q tensorflow-gpu==1.14"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3LApDa61AeR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "import numpy as np\n",
        "np.random.seed(seed=0)\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.contrib.eager as tfe\n",
        "\n",
        "tf.enable_eager_execution()\n",
        "tf.set_random_seed(0)\n",
        "\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import time\n",
        "\n",
        "from IPython.display import clear_output\n",
        "\n",
        "results_dir = './'\n",
        "model_dir = './models/'\n",
        "\n",
        "try:\n",
        "  step_counter = tf.train.create_global_step()\n",
        "except ValueError:\n",
        "  step_counter.assign(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GqvOhJ5_DhY",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title CelebA dataset download\n",
        "#https://gist.github.com/charlesreid1/4f3d676b33b95fce83af08e4ec261822\n",
        "import requests\n",
        "\n",
        "def download_file_from_google_drive(id, destination):\n",
        "  def get_confirm_token(response):\n",
        "    for key, value in response.cookies.items():\n",
        "      if key.startswith('download_warning'):\n",
        "        return value\n",
        "\n",
        "    return None\n",
        "\n",
        "  def save_response_content(response, destination):\n",
        "    CHUNK_SIZE = 32768\n",
        "\n",
        "    with open(destination, \"wb\") as f:\n",
        "      for chunk in response.iter_content(CHUNK_SIZE):\n",
        "        if chunk: # filter out keep-alive new chunks\n",
        "          f.write(chunk)\n",
        "\n",
        "  URL = \"https://docs.google.com/uc?export=download\"\n",
        "\n",
        "  session = requests.Session()\n",
        "\n",
        "  response = session.get(URL, params={'id': id }, stream=True)\n",
        "  token = get_confirm_token(response)\n",
        "\n",
        "  if token:\n",
        "    params = {'id': id, 'confirm': token}\n",
        "    response = session.get(URL, params = params, stream = True)\n",
        "\n",
        "  save_response_content(response, destination)\n",
        "\n",
        "files = {'readme.txt': '0B7EVK8r0v71pOXBhSUdJWU1MYUk',\n",
        "         'celeba.zip': '0B7EVK8r0v71pZjFTYXZWM3FlRnM',\n",
        "         'list_landmarks_align_celeba.txt': '0B7EVK8r0v71pd0FJY3Blby1HUTQ',\n",
        "         'list_attr_celeba.txt': '0B7EVK8r0v71pblRyaVFSWGxPY0U',\n",
        "         'list_eval_partition.txt': '0B7EVK8r0v71pY0NSMzRuSXJEVkk'}\n",
        "\n",
        "for filename in files:\n",
        "  download_file_from_google_drive(files[filename], filename)\n",
        "\n",
        "commands = ['unzip -oq celeba.zip > /dev/null',\n",
        "            \"sed -i '1d' list_landmarks_align_celeba.txt\",\n",
        "            \"sed -i 's/  */ /g' list_landmarks_align_celeba.txt\",\n",
        "            \"sed -i '1d' list_attr_celeba.txt\",\n",
        "            \"sed -i 's/  */ /g' list_attr_celeba.txt\"]\n",
        "\n",
        "for command in commands:\n",
        "  get_ipython().system_raw(command)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "012FjUK6EZwm",
        "colab_type": "text"
      },
      "source": [
        "The CelebA dataset is made available in a Google Drive folder which is subject to daily quotas. It might be temporarily unavailable until the quotas are replenished."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUDVogpGH7-0",
        "colab_type": "text"
      },
      "source": [
        "# Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cU1Tnshg-62Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "attribute = 'Attractive'\n",
        "iterations = 10\n",
        "\n",
        "landmarks_df = pd.read_csv('list_landmarks_align_celeba.txt', sep=' ')\n",
        "landmarks_df.iloc[:, 0::2] = landmarks_df.iloc[:, 0::2]/178\n",
        "landmarks_df.iloc[:, 1::2] = landmarks_df.iloc[:, 1::2]/218\n",
        "\n",
        "attributes_df = pd.read_csv('list_attr_celeba.txt', sep=' ')\n",
        "attributes_df.columns = ['name'] + list(attributes_df.columns)[:-1]\n",
        "attributes_df.set_index('name', inplace=True)\n",
        "attributes_df = attributes_df[[attribute]]\n",
        "attributes_df.replace(to_replace=-1, value=0, inplace=True)\n",
        "\n",
        "eval_df = pd.read_csv('list_eval_partition.txt', sep=' ', names=['name', 'set'])\n",
        "eval_df.set_index('name', inplace=True)\n",
        "\n",
        "landmarks_train = landmarks_df[eval_df['set'] == 0].sample(n=10000,\n",
        "                                                           random_state=0)\n",
        "attributes_train = attributes_df.loc[landmarks_train.index]\n",
        "\n",
        "landmarks_val = landmarks_df[eval_df['set'] == 1].sample(n=10000,\n",
        "                                                         random_state=0)\n",
        "attributes_val = attributes_df.loc[landmarks_val.index]\n",
        "\n",
        "landmarks_test = landmarks_df[eval_df['set'] == 2].sample(n=10000,\n",
        "                                                          random_state=0)\n",
        "attributes_test = attributes_df.loc[landmarks_test.index]\n",
        "\n",
        "def images(filenames):\n",
        "  for filename in filenames:\n",
        "    image = tf.read_file('img_align_celeba/' + filename)\n",
        "    image = tf.image.decode_jpeg(image)\n",
        "    image = tf.image.resize_bilinear(tf.expand_dims(image, 0), [40, 40])\n",
        "    if image.shape[3].value == 3:\n",
        "      image = tf.image.rgb_to_grayscale(image)\n",
        "    try:\n",
        "      result = tf.concat([result, image], axis=0)\n",
        "    except NameError:\n",
        "      result = image\n",
        "  return tf.constant(result/255, dtype=tf.float32)\n",
        "\n",
        "x_train = images(landmarks_train.index)\n",
        "x_val = images(landmarks_val.index)\n",
        "x_test = images(landmarks_test.index)\n",
        "\n",
        "y_train = tf.constant(np.array(attributes_train), dtype=tf.float32)\n",
        "y_val = tf.constant(np.array(attributes_val), dtype=tf.float32)\n",
        "y_test = tf.constant(np.array(attributes_test), dtype=tf.float32)\n",
        "\n",
        "z_train = tf.constant(np.array(landmarks_train), dtype=tf.float32)\n",
        "\n",
        "ds_train = tf.data.Dataset.from_tensor_slices((x_train, y_train, z_train))\n",
        "ds_train = ds_train.shuffle(1000).batch(100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3VA1EhPTWro",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Visualization of examples\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.cm as cm\n",
        "import matplotlib.pyplot as plt\n",
        "matplotlib.rcParams['figure.figsize'] = 12, 4\n",
        "\n",
        "def show(element, title=True):  \n",
        "  image = element[0].numpy()\n",
        "  if len(element) > 1:\n",
        "    lf = element[1]\n",
        "    c = element[2]\n",
        "    plt.plot(lf[0::2]*40,\n",
        "             lf[1::2]*40,\n",
        "             marker='o',\n",
        "             markerfacecolor='g',\n",
        "             markeredgecolor='g',\n",
        "             markersize=5,\n",
        "             linestyle = 'None')\n",
        "    if title:\n",
        "      plt.title('%s: %s' % (attribute.replace('_', ' '),\n",
        "                            {1: 'Yes', 0: 'No'}[c[0]]),\n",
        "                fontdict={'weight': 'bold'})\n",
        "  if len(element) > 3:\n",
        "    pred = element[3]\n",
        "    plt.plot(pred[0::2]*40,\n",
        "         pred[1::2]*40,\n",
        "         marker='o',\n",
        "         markerfacecolor='r',\n",
        "         linestyle = 'None')\n",
        "  plt.imshow(image.reshape([40, 40]), cmap=cm.gray)\n",
        "\n",
        "\n",
        "  plt.axis('off')\n",
        "  plt.grid(False)\n",
        "\n",
        "gs = matplotlib.gridspec.GridSpec(1, 4, width_ratios=[178, 218, 178, 218]) \n",
        "i = 20\n",
        "ax0 = plt.subplot(gs[0])\n",
        "filename = landmarks_train.index[i]\n",
        "image = tf.read_file('img_align_celeba/' + filename)\n",
        "image = tf.image.decode_jpeg(image)\n",
        "ax0.imshow(image)\n",
        "ax0.axis('off')\n",
        "ax0.grid(False)\n",
        "\n",
        "ax1 = plt.subplot(gs[1])\n",
        "image = x_train[i, :, : , :]\n",
        "lf = np.array(landmarks_train)[i, :]\n",
        "c = np.array(attributes_train)[i, :]\n",
        "show((image, lf, c), title=False)\n",
        "\n",
        "i = 105\n",
        "ax0 = plt.subplot(gs[2])\n",
        "filename = landmarks_train.index[i]\n",
        "image = tf.read_file('img_align_celeba/' + filename)\n",
        "image = tf.image.decode_jpeg(image)\n",
        "ax0.imshow(image)\n",
        "ax0.axis('off')\n",
        "ax0.grid(False)\n",
        "\n",
        "ax1 = plt.subplot(gs[3])\n",
        "image = x_train[i, :, : , :]\n",
        "lf = np.array(landmarks_train)[i, :]\n",
        "c = np.array(attributes_train)[i, :]\n",
        "show((image, lf, c), title=False)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('faces.png', bbox_inches='tight')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNln85tHIQMw",
        "colab_type": "text"
      },
      "source": [
        "# Model architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0U8uDRFTKUL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FaceModel(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(FaceModel, self).__init__()\n",
        "    kwargs = {'activation': 'relu'}\n",
        "    self.conv1 = tf.keras.layers.Conv2D(16, 5, **kwargs)\n",
        "    self.conv2 = tf.keras.layers.Conv2D(48, 3, **kwargs)\n",
        "    self.conv3 = tf.keras.layers.Conv2D(64, 3, **kwargs)\n",
        "    self.conv4 = tf.keras.layers.Conv2D(64, 2, **kwargs)\n",
        "    self.max = tf.keras.layers.MaxPooling2D()\n",
        "    self.flatten = tf.keras.layers.Flatten()\n",
        "    self.dense1 = tf.keras.layers.Dense(100, **kwargs)\n",
        "    self.dense2_primary = tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    self.dense2_auxiliary = tf.keras.layers.Dense(len(landmarks_df.columns),\n",
        "                                                  activation='sigmoid')\n",
        "  \n",
        "  def call(self, img):\n",
        "    x = self.max(self.conv1(img))\n",
        "    x = self.max(self.conv2(x))\n",
        "    x = self.max(self.conv3(x))\n",
        "    x = self.conv4(x)\n",
        "    x = self.flatten(x)\n",
        "    x = self.dense1(x)\n",
        "    return self.dense2_primary(x), self.dense2_auxiliary(x)\n",
        "\n",
        "def cross_entropy(y, y_hat):\n",
        "  eps = 1e-6\n",
        "  return -tf.math.reduce_mean(y*tf.math.log((1 - eps)*y_hat + eps)\n",
        "                              + (1-y)*tf.math.log((1 - eps)*(1 - y_hat) + eps))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXoFrSeMlYql",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "if not os.path.exists(model_dir):\n",
        "  os.mkdir(model_dir)\n",
        "  for i in range(iterations):\n",
        "    model = FaceModel()\n",
        "    _ = model(x_val[:1, :, : ,:])\n",
        "    model.save_weights(model_dir + 'model_%i.h5' % i)\n",
        "  del model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eH51bRs9Ywxg",
        "colab_type": "text"
      },
      "source": [
        "# Experiments with *Projection*, *Unweighted cosine*, *Weighted cosine* and *Orthogonal*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtPDABQzYPz4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def censored_vector(u, v, mode):\n",
        "  \"\"\"Adjusts the auxiliary loss gradient\n",
        "  \n",
        "  Adjusts the auxiliary loss gradient before adding it to the primary loss\n",
        "  gradient and using a gradient descent-based method\n",
        "  \n",
        "  Args:\n",
        "    u: A tensorflow variable representing the auxiliary loss gradient\n",
        "    v: A tensorflow variable representing the primary loss gradient\n",
        "    mode: The method used for the adjustment:\n",
        "      - Single task: the auxiliary loss gradient is ignored\n",
        "      - Multitask: the auxiliary loss gradient is kept as it is\n",
        "      - Unweighted cosine: cf. https://arxiv.org/abs/1812.02224\n",
        "      - Weighted cosine: cf. https://arxiv.org/abs/1812.02224\n",
        "      - Orthogonal: https://arxiv.org/abs/1801.07593\n",
        "      - Projection: cf. ICML submission\n",
        "    \n",
        "  Returns:\n",
        "    A tensorflow variable representing the adjusted auxiliary loss gradient\n",
        "  \"\"\"\n",
        "  if mode == 'Single task' or u is None:\n",
        "    return 0  \n",
        "  if mode == 'Multitask' or v is None:\n",
        "    return u\n",
        "  if len(u.shape.as_list()) == 1:\n",
        "    u_dot_v, l_u, l_v = tf.reduce_sum(u*v), tf.norm(u), tf.norm(v)\n",
        "  else:\n",
        "    a, b = tf.reshape(u, [-1]), tf.reshape(v, [-1])\n",
        "    u_dot_v, l_u, l_v = tf.reduce_sum(a*b), tf.norm(a), tf.norm(b)\n",
        "  if l_u.numpy() == 0 or l_v.numpy() == 0:\n",
        "    return u\n",
        "  if mode == 'Unweighted cosine':\n",
        "    return u if u_dot_v > 0 else tf.zeros_like(u)\n",
        "  if mode == 'Weighted cosine':\n",
        "    return tf.maximum(u_dot_v, 0)*u/l_u/l_v\n",
        "  if mode == 'Projection':\n",
        "    return u - tf.minimum(u_dot_v, 0)*v/l_v/l_v\n",
        "  if mode == 'Orthogonal':\n",
        "    return u - u_dot_v*v/l_v/l_v\n",
        "\n",
        "def combined_grads(primary_grad,\n",
        "                   average_primary_grad,\n",
        "                   auxiliary_grad,\n",
        "                   mode,\n",
        "                   overall=False,\n",
        "                   lam=1):\n",
        "  \"\"\"Combines auxiliary loss gradients and primary loss gradients\n",
        "  \n",
        "  Combines a sequence of auxiliary loss gradients and a sequence of primary\n",
        "  loss gradients before performing a gradient descent step\n",
        "  \n",
        "  Args:\n",
        "    primary_grad: A list of tensorflow variables corresponding to the primary\n",
        "    loss gradient for the network's Keras variables\n",
        "    average_primary_grad: A list of tensorflow variables corresponding to\n",
        "    exponential moving averages of the elements above\n",
        "    auxiliary_grad: A list of tensorflow variables corresponding to the\n",
        "    auxiliary loss gradient for the network's Keras variables\n",
        "    mode: The method used for the adjustment:\n",
        "      - Single task: the auxiliary loss gradient is ignored\n",
        "      - Multitask: the auxiliary loss gradient is kept as it is\n",
        "      - Unweighted cosine: cf. https://arxiv.org/abs/1812.02224\n",
        "      - Weighted cosine: cf. https://arxiv.org/abs/1812.02224\n",
        "      - Orthogonal: https://arxiv.org/abs/1801.07593\n",
        "      - Projection: cf. ICML submission\n",
        "    overall: True if the transformation takes place at the level of the whole\n",
        "    parameter vector, i.e. the concatenation of all the Keras variables of the\n",
        "    network\n",
        "    lambda: Float balancing the primary loss and the auxiliary loss\n",
        "    \n",
        "  Returns:\n",
        "    A list of tensorflow variables combining the primary loss gradients and the\n",
        "    auxiliary loss gradients and that can directly be used for the next gradient\n",
        "    descent step\n",
        "  \"\"\"\n",
        "  result = [0]*len(primary_grad)\n",
        "  a = tf.constant([], dtype=tf.float32)\n",
        "  aa = tf.constant([], dtype=tf.float32)\n",
        "  b = tf.constant([], dtype=tf.float32)\n",
        "  shapes = []\n",
        "  for i in range(len(primary_grad)):\n",
        "    if auxiliary_grad[i] is None or mode == 'Single task':\n",
        "      result[i] = primary_grad[i]\n",
        "    elif primary_grad[i] is None:\n",
        "      result[i] = lam*auxiliary_grad[i]\n",
        "    elif mode == 'Multitask':\n",
        "      result[i] = primary_grad[i] + lam*auxiliary_grad[i]\n",
        "    elif not overall:\n",
        "      if average_primary_grad is None:\n",
        "        result[i] = (primary_grad[i]\n",
        "                     + lam*censored_vector(auxiliary_grad[i],\n",
        "                                           primary_grad[i],\n",
        "                                           mode))\n",
        "      else:\n",
        "        result[i] = (primary_grad[i]\n",
        "                     + lam*censored_vector(auxiliary_grad[i],\n",
        "                                           average_primary_grad[i],\n",
        "                                           mode))\n",
        "    else:\n",
        "      a = tf.concat([a, tf.reshape(primary_grad[i], [-1])], axis=0)\n",
        "      if average_primary_grad is not None:\n",
        "        aa = tf.concat([aa, tf.reshape(average_primary_grad[i], [-1])], axis=0)\n",
        "      b = tf.concat([b, tf.reshape(auxiliary_grad[i], [-1])], axis=0)\n",
        "      shapes.append((primary_grad[i].shape,\n",
        "                     np.product(primary_grad[i].shape.as_list()),\n",
        "                     i))\n",
        "\n",
        "  if len(shapes) > 0:\n",
        "    if average_primary_grad is None:\n",
        "      c = a + lam*censored_vector(b, a, mode)\n",
        "    else:\n",
        "      c = a + lam*censored_vector(b, aa, mode)\n",
        "    start = 0\n",
        "    for i in range(len(shapes)):\n",
        "      shape, length, index = shapes[i]\n",
        "      result[index] = tf.reshape(c[start:start+length], shape)\n",
        "      start += length\n",
        "  return result\n",
        "\n",
        "def train_iteration(model,\n",
        "                    average_primary_grad,\n",
        "                    alpha,\n",
        "                    optimizer,\n",
        "                    writer,\n",
        "                    step_counter,\n",
        "                    mode,\n",
        "                    overall=False,\n",
        "                    lam=1):\n",
        "  \"\"\"Trains the model for one epoch\n",
        "   \n",
        "  Args:\n",
        "    model: The Keras model being trained\n",
        "    average_primary_grad: An exponential moving average of the main loss gradient for each variable\n",
        "    alpha: The factor for the exponential moving average\n",
        "    optimizer: The optimizer being used\n",
        "    writer: The writer collecting summaries\n",
        "    step_counter: The global counter used by the optimizer\n",
        "    mode: The method used for adjusting the auxiliary loss gradient:\n",
        "      - Single task: the auxiliary loss gradient is ignored\n",
        "      - Multitask: the auxiliary loss gradient is kept as it is\n",
        "      - Unweighted cosine: cf. https://arxiv.org/abs/1812.02224\n",
        "      - Weighted cosine: cf. https://arxiv.org/abs/1812.02224\n",
        "      - Orthogonal: https://arxiv.org/abs/1801.07593\n",
        "      - Projection: cf. ICML submission\n",
        "    overall: A boolean indicating whether the previous method should be applied to the whole parameter vector\n",
        "    lam: The weight of the auxiliary task  \n",
        "    \n",
        "  Returns:\n",
        "    The updated value of the exponential moving average of the main loss gradient for each variable\n",
        "  \"\"\"\n",
        "  with writer.as_default(), tf.contrib.summary.always_record_summaries():\n",
        "    for x, y, z in ds_train.make_one_shot_iterator():\n",
        "      with tf.GradientTape(persistent=True) as tape:\n",
        "        y_hat, z_hat = model(x)\n",
        "        primary_loss = cross_entropy(y, y_hat)\n",
        "        if mode != 'Single task':\n",
        "          auxiliary_loss = tf.reduce_mean((z_hat-z)**2)\n",
        "      tf.contrib.summary.scalar('primary_loss', primary_loss)\n",
        "     \n",
        "      primary_grad = tape.gradient(primary_loss, model.variables)\n",
        "      if mode == 'Single task':\n",
        "        optimizer.apply_gradients(zip(primary_grad, model.variables),\n",
        "                                  global_step=step_counter)\n",
        "      else:\n",
        "        tf.contrib.summary.scalar('auxiliary_loss', auxiliary_loss)\n",
        "        auxiliary_grad = tape.gradient(auxiliary_loss, model.variables)\n",
        "        \n",
        "        if alpha != 1:\n",
        "          if average_primary_grad is None:\n",
        "            average_primary_grad = primary_grad\n",
        "          else:\n",
        "            for i in range(len(average_primary_grad)):\n",
        "              if primary_grad[i] is not None:\n",
        "                average_primary_grad[i] = ((1 - alpha)*average_primary_grad[i]\n",
        "                                           + alpha*primary_grad[i])        \n",
        "        \n",
        "        grad = combined_grads(primary_grad,\n",
        "                              average_primary_grad,\n",
        "                              auxiliary_grad,\n",
        "                              mode,\n",
        "                              overall=overall,\n",
        "                              lam=lam)\n",
        "        optimizer.apply_gradients(zip(grad, model.variables),\n",
        "                                  global_step=step_counter)\n",
        "  return average_primary_grad\n",
        "\n",
        "def get_metrics(dataset,\n",
        "                model,\n",
        "                writer,\n",
        "                step_counter):\n",
        "  x, y = (x_val, y_val) if dataset == 'val' else (x_test, y_test)\n",
        "  with writer.as_default(), tf.contrib.summary.always_record_summaries():\n",
        "    y_hat = model(x)[0]\n",
        "    primary_loss = cross_entropy(y, y_hat)\n",
        "    tf.contrib.summary.scalar('primary_loss', primary_loss)\n",
        "    acc = sklearn.metrics.accuracy_score(y.numpy(), y_hat.numpy()>0.5)\n",
        "    tf.contrib.summary.scalar('accuracy', acc)\n",
        "    fpr, tpr, _ = sklearn.metrics.roc_curve(y.numpy(), y_hat.numpy())\n",
        "    auc = sklearn.metrics.auc(fpr, tpr)\n",
        "    tf.contrib.summary.scalar('AUC', auc)\n",
        "  return primary_loss.numpy(), acc, auc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2HlhKpHh-Z2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_experiment(name, model, alpha, mode, overall, lam, output, lr):\n",
        "  \"\"\"Trains the model until early stopping\n",
        "   \n",
        "  Args:\n",
        "    name: The name to be used for the Tensorboard log files\n",
        "    model: The Keras model being trained\n",
        "    alpha: The factor for the exponential moving average\n",
        "    mode: The method used for adjusting the auxiliary loss gradient:\n",
        "      - Single task: the auxiliary loss gradient is ignored\n",
        "      - Multitask: the auxiliary loss gradient is kept as it is\n",
        "      - Unweighted cosine: cf. https://arxiv.org/abs/1812.02224\n",
        "      - Weighted cosine: cf. https://arxiv.org/abs/1812.02224\n",
        "      - Orthogonal: https://arxiv.org/abs/1801.07593\n",
        "      - Projection: cf. ICML submission\n",
        "    overall: A boolean indicating whether the previous method should be applied to the whole parameter vector\n",
        "    lam: The weight of the auxiliary task\n",
        "    output: The current output printed for the user during training\n",
        "    lr: The learning rate for the optimizer\n",
        "    \n",
        "  Returns:\n",
        "    The performance metrics on the test set for the best model on the validation set\n",
        "  \"\"\"\n",
        "  train_writer = tf.contrib.summary.create_file_writer('./log/train/' + name,\n",
        "                                                       flush_millis=10000)\n",
        "  val_writer = tf.contrib.summary.create_file_writer('./log/val/' + name,\n",
        "                                                       flush_millis=10000)\n",
        "  test_writer = tf.contrib.summary.create_file_writer('./log/test/' + name,\n",
        "                                                      flush_millis=10000)\n",
        "  step_counter.assign(0)\n",
        "  optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
        "  checkpoint_dir = 'model_celeba'\n",
        "  shutil.rmtree(checkpoint_dir, ignore_errors=True)\n",
        "  checkpoint_prefix = os.path.join(checkpoint_dir, 'model.ckpt')\n",
        "  root = tf.contrib.eager.Checkpoint(optimizer=optimizer,\n",
        "                                     model=model,\n",
        "                                     optimizer_step=step_counter)\n",
        "  \n",
        "  average_primary_grad = None\n",
        "  iteration, not_better, best_auc = 1, 0, 0\n",
        "  while not_better < 10:\n",
        "    average_primary_grad = train_iteration(model,\n",
        "                                           average_primary_grad,\n",
        "                                           alpha,\n",
        "                                           optimizer,\n",
        "                                           train_writer,\n",
        "                                           step_counter,\n",
        "                                           mode,\n",
        "                                           overall=overall,\n",
        "                                           lam=lam)\n",
        "    val_loss, val_acc, val_auc = get_metrics('val',\n",
        "                                             model,\n",
        "                                             val_writer,\n",
        "                                             step_counter)\n",
        "    clear_output()\n",
        "    print(output)\n",
        "    print(iteration, val_loss, val_acc, val_auc)\n",
        "    if val_auc > best_auc:\n",
        "      not_better, best_auc = 0, val_auc\n",
        "      root.save(file_prefix=checkpoint_prefix)\n",
        "    else:\n",
        "      not_better += 1\n",
        "    iteration += 1\n",
        "\n",
        "  root.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "  metrics = get_metrics('test', model, test_writer, step_counter)\n",
        "  return (best_auc, *metrics)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LF52tk5h3RAa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_experiments(configs, filename):\n",
        "  \"\"\"Trains models for a series of configurations\n",
        "   \n",
        "  Args:\n",
        "    configs: The combinations of hyper-parameters to use in the experiments\n",
        "    filename: The name of the file used for recording the experiments' results\n",
        "    \n",
        "  Returns:\n",
        "    Nothing\n",
        "  \"\"\"\n",
        "  already_done = !cat {filename.replace(' ', '\\ ')} | wc -l\n",
        "  already_done = int(already_done[0])\n",
        "\n",
        "  output = ''\n",
        "\n",
        "  for iteration, mode, overall, lam, alpha, lr in configs[already_done:]:\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    model = FaceModel()\n",
        "    _ = model(x_val[:1, :, : ,:])\n",
        "    model.load_weights(model_dir + 'model_%i.h5' % iteration)\n",
        "\n",
        "    output += 'Iteration #%d: %s (overall: %s, %d)\\n' % (iteration,\n",
        "                                                          mode,\n",
        "                                                          overall,\n",
        "                                                          lam)\n",
        "    name = '%s-%s-%s-%d-%f' % (iteration, mode, overall, lam, alpha)\n",
        "    best_auc, test_loss, test_acc, test_auc = run_experiment(name,\n",
        "                                                   model,\n",
        "                                                   alpha,\n",
        "                                                   mode,\n",
        "                                                   overall,\n",
        "                                                   lam,\n",
        "                                                   output,\n",
        "                                                   lr)\n",
        "    template = 'Loss: %.3f, accuracy: %.1f%%, AUC: %.3f (%d seconds)\\n\\n'\n",
        "    output += template % (test_loss, test_acc*100, test_auc, time.time()-start)\n",
        "    line = \"%s,%s,%f,%d,%f,%f,%f,%f,%f\" % (mode,\n",
        "                                           overall,\n",
        "                                           alpha,\n",
        "                                           lam,\n",
        "                                           lr,\n",
        "                                           best_auc,\n",
        "                                           test_loss,\n",
        "                                           test_acc,\n",
        "                                           test_auc)\n",
        "    with open(filename, 'a') as file:\n",
        "      file.write(f'\\n{line}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1enOEgNShur",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filename = results_dir + 'real_dataset_experiments.csv'\n",
        "header = 'mode,overall,alpha,lam,lr,best_auc,test_loss,test_acc,test_auc'\n",
        "\n",
        "if not os.path.isfile(filename):\n",
        "    with open(filename, 'w') as file:\n",
        "        file.write(header)\n",
        "\n",
        "configs = [('Single task', True, 0, 1)]\n",
        "\n",
        "configs += [('Multitask', True, lam, 1)\n",
        "           for lam in [300, 1000, 3000, 10000, 30000]]\n",
        "\n",
        "configs += [('Projection', overall, lam, 1)\n",
        "           for overall in [True, False]\n",
        "           for lam in [300, 1000, 3000, 10000, 30000]]\n",
        "\n",
        "configs += [('Projection', True, lam, 0.01)\n",
        "           for lam in [300, 1000, 3000, 10000, 30000]]\n",
        "\n",
        "configs = [(iteration, mode, overall, lam, alpha, 1e-3)\n",
        "           for iteration in range(iterations)\n",
        "           for mode, overall, lam, alpha in configs]\n",
        "\n",
        "configs2 = [(mode, overall, lam, 1)\n",
        "           for overall in [True, False]\n",
        "           for lam in [300, 1000, 3000, 10000, 30000]\n",
        "           for mode in ['Unweighted cosine', 'Weighted cosine']]\n",
        "\n",
        "configs2 = [(iteration, mode, overall, lam, alpha, 1e-3)\n",
        "           for iteration in range(iterations)\n",
        "           for mode, overall, lam, alpha in configs2]\n",
        "\n",
        "configs3 = [(mode, overall, lam, alpha)\n",
        "           for overall in [True, False]\n",
        "           for lam in [300, 1000, 3000, 10000, 30000]\n",
        "           for mode in ['Orthogonal']\n",
        "           for alpha in [0.01, 1]]\n",
        "\n",
        "configs3 = [(iteration, mode, overall, lam, alpha, 1e-3)\n",
        "           for iteration in range(iterations)\n",
        "           for mode, overall, lam, alpha in configs3]\n",
        "\n",
        "configs = configs + configs2 + configs3\n",
        "_ = run_experiments(configs, filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etAqRGHMZOsH",
        "colab_type": "text"
      },
      "source": [
        "# Experiments with *Adaptive auxiliary task weighting*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZtlJ_pvc2L2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_iteration_OL_AUX(model,\n",
        "                           accumulator,\n",
        "                           horizon,\n",
        "                           lr,\n",
        "                           beta,\n",
        "                           optimizer,\n",
        "                           writer,\n",
        "                           step_counter,\n",
        "                           log=False,\n",
        "                           lam=1):\n",
        "  \"\"\"Trains the model for one epoch with Adaptive auxiliary task weighting\n",
        "  (https://papers.nips.cc/paper/8724-adaptive-auxiliary-task-weighting-for-reinforcement-learning)\n",
        "   \n",
        "  Args:\n",
        "    model: The Keras model being trained\n",
        "    accumulator: The accumulated values of the dot products of the main loss gradients and the auxiliary loss gradients\n",
        "    horizon: The horizon defined in the Adaptive auxiliary task weighting method\n",
        "    lr: The learning rate of the optimizer\n",
        "    beta: The learning rate for adjusting the weight of the auxiliary task\n",
        "    optimizer: The optimizer being used\n",
        "    writer: The writer collecting summaries\n",
        "    step_counter: The global counter used by the optimizer\n",
        "    log: A boolean indicating whether to apply a logarithm function on the losses\n",
        "    lam: The weight of the auxiliary task  \n",
        "    \n",
        "  Returns:\n",
        "    The updated value of the weight of the auxiliary task and the accumulator\n",
        "  \"\"\"\n",
        "  with writer.as_default(), tf.contrib.summary.always_record_summaries():\n",
        "    for x, y, z in ds_train.make_one_shot_iterator():\n",
        "      with tf.GradientTape(persistent=True) as tape:\n",
        "        y_hat, z_hat = model(x)\n",
        "        if not log:\n",
        "          primary_loss = cross_entropy(y, y_hat)\n",
        "          auxiliary_loss = tf.reduce_mean((z_hat-z)**2)\n",
        "        else:\n",
        "          primary_loss = tf.log(cross_entropy(y, y_hat))\n",
        "          auxiliary_loss = tf.log(tf.reduce_mean((z_hat-z)**2))\n",
        "      tf.contrib.summary.scalar('primary_loss', primary_loss)\n",
        "      tf.contrib.summary.scalar('auxiliary_loss', auxiliary_loss)\n",
        "\n",
        "      primary_grad = tape.gradient(primary_loss, model.variables)\n",
        "      auxiliary_grad = tape.gradient(auxiliary_loss, model.variables)\n",
        "      \n",
        "      primary_grad = [u if u is not None else 0 for u in primary_grad]\n",
        "      auxiliary_grad = [u if u is not None else 0 for u in auxiliary_grad]\n",
        "      \n",
        "      total_grad = [primary_grad[i] + lam*auxiliary_grad[i] for i in range(len(primary_grad))]\n",
        "      optimizer.apply_gradients(zip(total_grad, model.variables),\n",
        "                                global_step=step_counter)\n",
        "      \n",
        "      for i in range(len(primary_grad)):\n",
        "        if primary_grad[i] != 0 and auxiliary_grad[i] != 0:\n",
        "          accumulator += tf.tensordot(primary_grad[i], auxiliary_grad[i], len(tf.shape(primary_grad[i]).numpy()))\n",
        "\n",
        "      if (step_counter.value().numpy() + 1) % horizon == 0:\n",
        "        lam -= lr*beta*accumulator\n",
        "        accumulator = 0\n",
        "\n",
        "  return accumulator, lam\n",
        "\n",
        "def run_experiment_OL_AUX(name, model, horizon, beta, lam, output, lr, log=False):\n",
        "  \"\"\"Trains the model until early stopping with Adaptive auxiliary task weighting\n",
        "  (https://papers.nips.cc/paper/8724-adaptive-auxiliary-task-weighting-for-reinforcement-learning)\n",
        "   \n",
        "  Args:\n",
        "    name: The name to be used for the Tensorboard log files\n",
        "    model: The Keras model being trained\n",
        "    horizon: The horizon defined in the Adaptive auxiliary task weighting method\n",
        "    beta: The learning rate for adjusting the weight of the auxiliary task\n",
        "    lam: The initial weight of the auxiliary task\n",
        "    output: The current output printed for the user during training\n",
        "    lr: The learning rate for the optimizer\n",
        "    log: A boolean indicating whether to apply a logarithm function on the losses\n",
        "    \n",
        "  Returns:\n",
        "    The performance metrics on the test set for the best model on the validation set and the final lam\n",
        "  \"\"\"\n",
        "  train_writer = tf.contrib.summary.create_file_writer('./log/train/' + name,\n",
        "                                                       flush_millis=10000)\n",
        "  val_writer = tf.contrib.summary.create_file_writer('./log/val/' + name,\n",
        "                                                       flush_millis=10000)\n",
        "  test_writer = tf.contrib.summary.create_file_writer('./log/test/' + name,\n",
        "                                                      flush_millis=10000)\n",
        "  step_counter.assign(0)\n",
        "  optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
        "  checkpoint_dir = 'model_celeba'\n",
        "  shutil.rmtree(checkpoint_dir, ignore_errors=True)\n",
        "  checkpoint_prefix = os.path.join(checkpoint_dir, 'model.ckpt')\n",
        "  root = tf.contrib.eager.Checkpoint(optimizer=optimizer,\n",
        "                                     model=model,\n",
        "                                     optimizer_step=step_counter)\n",
        "  \n",
        "  accumulator = 0\n",
        "  iteration, not_better, best_auc = 1, 0, 0\n",
        "  while not_better < 10:\n",
        "    accumulator, lam = train_iteration_OL_AUX(model,\n",
        "                                              accumulator,\n",
        "                                              horizon,\n",
        "                                              lr,\n",
        "                                              beta,\n",
        "                                              optimizer,\n",
        "                                              train_writer,\n",
        "                                              step_counter,\n",
        "                                              log=False,\n",
        "                                              lam=lam)\n",
        "    val_loss, val_acc, val_auc = get_metrics('val',\n",
        "                                             model,\n",
        "                                             val_writer,\n",
        "                                             step_counter)\n",
        "    clear_output()\n",
        "    print(output)\n",
        "    print(iteration, val_loss, val_acc, val_auc)\n",
        "    if val_auc > best_auc:\n",
        "      not_better, best_auc = 0, val_auc\n",
        "      root.save(file_prefix=checkpoint_prefix)\n",
        "    else:\n",
        "      not_better += 1\n",
        "    iteration += 1\n",
        "\n",
        "  root.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "  metrics = get_metrics('test', model, test_writer, step_counter)\n",
        "  return (best_auc, *metrics, lam)\n",
        "\n",
        "def run_experiments_OL_AUX(configs, filename):\n",
        "  \"\"\"Trains models for a series of configurations\n",
        "   \n",
        "  Args:\n",
        "    configs: The combinations of hyper-parameters to use in the experiments\n",
        "    filename: The name of the file used for recording the experiments' results\n",
        "    \n",
        "  Returns:\n",
        "    Nothing\n",
        "  \"\"\"\n",
        "  already_done = !cat {filename.replace(' ', '\\ ')} | wc -l\n",
        "  already_done = int(already_done[0])\n",
        "\n",
        "  output = ''\n",
        "\n",
        "  for iteration, horizon, beta, lam, lr, log in configs[already_done:]:\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    model = FaceModel()\n",
        "    _ = model(x_val[:1, :, : ,:])\n",
        "    model.load_weights(model_dir + 'model_%i.h5' % iteration)\n",
        "\n",
        "    output += 'Iteration #%d, beta: %f, lam: %d\\n' % (iteration,\n",
        "                                                         beta,\n",
        "                                                         lam)\n",
        "    name = '%s-%d-%d-%d-%f' % (iteration, horizon, beta, lam, lr)\n",
        "    best_auc, test_loss, test_acc, test_auc, final_lam = run_experiment_OL_AUX(name,\n",
        "                                                                               model,\n",
        "                                                                               horizon,\n",
        "                                                                               beta,\n",
        "                                                                               lam,\n",
        "                                                                               output,\n",
        "                                                                               lr,\n",
        "                                                                               log)\n",
        "    template = 'Loss: %.3f, accuracy: %.1f%%, AUC: %.3f, final Lam: %.3f (%d seconds)\\n\\n'\n",
        "    output += template % (test_loss, test_acc*100, test_auc, final_lam, time.time()-start)\n",
        "    line = \"%f,%f,%f,%f,%s,%f,%f,%f,%f\" % (lam,\n",
        "                                        final_lam,\n",
        "                                        beta,\n",
        "                                        lr,\n",
        "                                        log,\n",
        "                                        best_auc,\n",
        "                                        test_loss,\n",
        "                                        test_acc,\n",
        "                                        test_auc)\n",
        "    with open(filename, 'a') as file:\n",
        "      file.write(f'\\n{line}')\n",
        "\n",
        "filename = results_dir + 'real_dataset_experiments2.csv'\n",
        "header = 'lam,final_lam,beta,lr,log,best_auc,test_loss,test_acc,test_auc'\n",
        "\n",
        "if not os.path.isfile(filename):\n",
        "    with open(filename, 'w') as file:\n",
        "        file.write(header)\n",
        "\n",
        "lams = [300, 1000, 3000, 10000, 30000]\n",
        "\n",
        "configs = [(iteration, 5, beta, lam, 1e-3, False)\n",
        "           for iteration in range(10)\n",
        "           for beta in [100, 1000, 10000, 100000]\n",
        "           for lam in lams]\n",
        "\n",
        "_ = run_experiments_OL_AUX(configs, filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PSTz7QTECgT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filename = results_dir + 'experiment-1.csv'\n",
        "header = 'lam,final_lam,beta,lr,log,best_auc,test_loss,test_acc,test_auc'\n",
        "\n",
        "if not os.path.isfile(filename):\n",
        "    with open(filename, 'w') as file:\n",
        "        file.write(header)\n",
        "\n",
        "lams = [300, 1000, 3000, 10000, 30000]\n",
        "\n",
        "configs = [(iteration, 5, beta, lam, 1e-3, True)\n",
        "           for iteration in range(10)\n",
        "           for beta in [100, 1000, 10000, 100000]\n",
        "           for lam in lams]\n",
        "\n",
        "_ = run_experiments_OL_AUX(configs, filename)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}