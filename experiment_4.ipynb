{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "experiment-4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYdS0uUqklP2",
        "colab_type": "text"
      },
      "source": [
        "Code derived from a [notebook](https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/4.4-overfitting-and-underfitting.ipynb) by [FranÃ§ois Chollet](https://github.com/fchollet)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "49PPQ0TbBX0v",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "!pip install -q tensorflow-gpu==2.0\n",
        "%load_ext tensorboard"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5pZ8A2liqvgk",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "models_dir = './models'\n",
        "results_dir = \"./\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uD9DNePMQ5NW",
        "colab_type": "text"
      },
      "source": [
        "#Dataset loading and pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QpzE4iqZtJly",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "NUM_WORDS = 10000\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.imdb.load_data(num_words=NUM_WORDS)\n",
        "\n",
        "def multi_hot_sequences(sequences, dimension):\n",
        "  # Create an all-zero matrix of shape (len(sequences), dimension)\n",
        "  results = np.zeros((len(sequences), dimension))\n",
        "  for i, word_indices in enumerate(sequences):\n",
        "    results[i, word_indices] = 1.0  # set specific indices of results[i] to 1s\n",
        "  return results\n",
        "\n",
        "y_train = y_train.astype('float32')\n",
        "y_test = y_test.astype('float32')\n",
        "y_train = np.expand_dims(y_train, 1)\n",
        "y_test = np.expand_dims(y_test, 1)\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=0)\n",
        "\n",
        "x_train = multi_hot_sequences(x_train, dimension=NUM_WORDS)\n",
        "x_val = multi_hot_sequences(x_val, dimension=NUM_WORDS)\n",
        "x_test = multi_hot_sequences(x_test, dimension=NUM_WORDS)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_val = x_val.astype('float32')\n",
        "\n",
        "ds_train = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "ds_train = ds_train.shuffle(1000).batch(512)\n",
        "\n",
        "plt.plot(x_train[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUsgfYC-Q1Pu",
        "colab_type": "text"
      },
      "source": [
        "#Model architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1k2Zr8P_g4_r",
        "colab_type": "code",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "def bigger_model():\n",
        "  return keras.Sequential([\n",
        "    # `input_shape` is only required here so that `.summary` works.\n",
        "    keras.layers.Dense(512, activation=tf.nn.relu, input_shape=(NUM_WORDS,)),\n",
        "    keras.layers.Dense(512, activation=tf.nn.relu),\n",
        "    keras.layers.Dense(1)\n",
        "  ])\n",
        "\n",
        "# Initializes a series of networks to be used in the experiments, so that each\n",
        "# combination of hyperparameters is tested with the same initial weights\n",
        "tf.random.set_seed(0)\n",
        "if not os.path.exists(models_dir):\n",
        "  os.mkdir(models_dir)\n",
        "  for i in range(50):\n",
        "    model = bigger_model()\n",
        "    _ = model(x_val[:1, :])\n",
        "    model.save_weights('%s/model_%i.h5' % (models_dir, i))\n",
        "  del model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oguFn0JxACya",
        "colab_type": "text"
      },
      "source": [
        "# Experiments with *Projection*, *Unweighted cosine*, *Weighted cosine* and *Orthogonal*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfVePRwgj1Vh",
        "colab_type": "code",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "def censored_vector(u, v, mode):\n",
        "  \"\"\"Adjusts the auxiliary loss gradient\n",
        "  \n",
        "  Adjusts the auxiliary loss gradient before adding it to the primary loss\n",
        "  gradient and using a gradient descent-based method\n",
        "  \n",
        "  Args:\n",
        "    u: A tensorflow variable representing the auxiliary loss gradient\n",
        "    v: A tensorflow variable representing the primary loss gradient\n",
        "    mode: The method used for the adjustment:\n",
        "      - Single task: the auxiliary loss gradient is ignored\n",
        "      - Multitask: the auxiliary loss gradient is kept as it is\n",
        "      - Unweighted cosine: cf. https://arxiv.org/abs/1812.02224\n",
        "      - Weighted cosine: cf. https://arxiv.org/abs/1812.02224\n",
        "      - Orthogonal: https://arxiv.org/abs/1801.07593\n",
        "      - Projection: cf. ICML submission\n",
        "    \n",
        "  Returns:\n",
        "    A tensorflow variable representing the adjusted auxiliary loss gradient\n",
        "  \"\"\"\n",
        "  if mode == 'Single task' or u is None:\n",
        "    return 0  \n",
        "  if mode == 'Multitask' or v is None:\n",
        "    return u\n",
        "  if len(u.shape.as_list()) == 1:\n",
        "    u_dot_v, l_u, l_v = tf.reduce_sum(u*v), tf.norm(u), tf.norm(v)\n",
        "  else:\n",
        "    a, b = tf.reshape(u, [-1]), tf.reshape(v, [-1])\n",
        "    u_dot_v, l_u, l_v = tf.reduce_sum(a*b), tf.norm(a), tf.norm(b)\n",
        "  if l_u.numpy() == 0 or l_v.numpy() == 0:\n",
        "    return u\n",
        "  if mode == 'Unweighted cosine':\n",
        "    return u if u_dot_v > 0 else tf.zeros_like(u)\n",
        "  if mode == 'Weighted cosine':\n",
        "    return tf.maximum(u_dot_v, 0)*u/l_u/l_v\n",
        "  if mode == 'Projection':\n",
        "    return u - tf.minimum(u_dot_v, 0)*v/l_v/l_v\n",
        "  if mode == 'Orthogonal':\n",
        "    return u - u_dot_v*v/l_v/l_v\n",
        "\n",
        "def combined_grads(primary_grad,\n",
        "                   average_primary_grad,\n",
        "                   auxiliary_grad,\n",
        "                   mode,\n",
        "                   overall=False,\n",
        "                   lam=1):\n",
        "  \"\"\"Combines auxiliary loss gradients and primary loss gradients\n",
        "  \n",
        "  Combines a sequence of auxiliary loss gradients and a sequence of primary\n",
        "  loss gradients before performing a gradient descent step\n",
        "  \n",
        "  Args:\n",
        "    primary_grad: A list of tensorflow variables corresponding to the primary\n",
        "    loss gradient for the network's Keras variables\n",
        "    average_primary_grad: A list of tensorflow variables corresponding to\n",
        "    exponential moving averages of the elements above\n",
        "    auxiliary_grad: A list of tensorflow variables corresponding to the\n",
        "    auxiliary loss gradient for the network's Keras variables\n",
        "    mode: The method used for the adjustment:\n",
        "      - Single task: the auxiliary loss gradient is ignored\n",
        "      - Multitask: the auxiliary loss gradient is kept as it is\n",
        "      - Unweighted cosine: cf. https://arxiv.org/abs/1812.02224\n",
        "      - Weighted cosine: cf. https://arxiv.org/abs/1812.02224\n",
        "      - Orthogonal: https://arxiv.org/abs/1801.07593\n",
        "      - Projection: cf. ICML submission\n",
        "    overall: True if the transformation takes place at the level of the whole\n",
        "    parameter vector, i.e. the concatenation of all the Keras variables of the\n",
        "    network\n",
        "    lambda: Float balancing the primary loss and the auxiliary loss\n",
        "    \n",
        "  Returns:\n",
        "    A list of tensorflow variables combining the primary loss gradients and the\n",
        "    auxiliary loss gradients and that can directly be used for the next gradient\n",
        "    descent step\n",
        "  \"\"\"\n",
        "  result = [0]*len(primary_grad)\n",
        "  a = tf.constant([], dtype=tf.float32)\n",
        "  aa = tf.constant([], dtype=tf.float32)\n",
        "  b = tf.constant([], dtype=tf.float32)\n",
        "  shapes = []\n",
        "  for i in range(len(primary_grad)):\n",
        "    if auxiliary_grad[i] is None or mode == 'Single task':\n",
        "      result[i] = tf.zeros_like(primary_grad[i])\n",
        "    elif primary_grad[i] is None or mode == 'Multitask':\n",
        "      result[i] = lam*auxiliary_grad[i]\n",
        "    elif not overall:\n",
        "      if average_primary_grad is None:\n",
        "        result[i] = lam*censored_vector(auxiliary_grad[i],\n",
        "                                        primary_grad[i],\n",
        "                                        mode)\n",
        "      else:\n",
        "        result[i] = lam*censored_vector(auxiliary_grad[i],\n",
        "                                        average_primary_grad[i],\n",
        "                                        mode)\n",
        "    else:\n",
        "      a = tf.concat([a, tf.reshape(primary_grad[i], [-1])], axis=0)\n",
        "      if average_primary_grad is not None:\n",
        "        aa = tf.concat([aa, tf.reshape(average_primary_grad[i], [-1])], axis=0)\n",
        "      b = tf.concat([b, tf.reshape(auxiliary_grad[i], [-1])], axis=0)\n",
        "      shapes.append((primary_grad[i].shape,\n",
        "                     np.product(primary_grad[i].shape.as_list()),\n",
        "                     i))\n",
        "\n",
        "  if len(shapes) > 0:\n",
        "    if average_primary_grad is None:\n",
        "      c = lam*censored_vector(b, a, mode)\n",
        "    else:\n",
        "      c = lam*censored_vector(b, aa, mode)\n",
        "    start = 0\n",
        "    for i in range(len(shapes)):\n",
        "      shape, length, index = shapes[i]\n",
        "      result[index] = tf.reshape(c[start:start+length], shape)\n",
        "      start += length\n",
        "  return result\n",
        "\n",
        "def train_iteration(model,\n",
        "                    average_primary_grad,\n",
        "                    alpha,\n",
        "                    optimizer,\n",
        "                    writer,\n",
        "                    mode,\n",
        "                    step,\n",
        "                    overall=False,\n",
        "                    lam=1):\n",
        "  \"\"\"Trains the model for one epoch\n",
        "   \n",
        "  Args:\n",
        "    model: The Keras model being trained\n",
        "    average_primary_grad: An exponential moving average of the main loss gradient for each variable\n",
        "    alpha: The factor for the exponential moving average\n",
        "    optimizer: The optimizer being used\n",
        "    writer: The writer collecting summaries\n",
        "    mode: The method used for adjusting the auxiliary loss gradient:\n",
        "      - Single task: the auxiliary loss gradient is ignored\n",
        "      - Multitask: the auxiliary loss gradient is kept as it is\n",
        "      - Unweighted cosine: cf. https://arxiv.org/abs/1812.02224\n",
        "      - Weighted cosine: cf. https://arxiv.org/abs/1812.02224\n",
        "      - Orthogonal: https://arxiv.org/abs/1801.07593\n",
        "      - Projection: cf. ICML submission\n",
        "    step: the number of mini-batches so far      \n",
        "    overall: A boolean indicating whether the previous method should be applied to the whole parameter vector\n",
        "    lam: The weight of the auxiliary task  \n",
        "    \n",
        "  Returns:\n",
        "    The updated value of the exponential moving average of the main loss gradient for each variable\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  with writer.as_default():\n",
        "    for x1, y1 in ds_train.__iter__():    \n",
        "      step += 1\n",
        "      with tf.GradientTape() as tape:\n",
        "        y1_hat = model(x1)\n",
        "        primary_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=y1, logits=y1_hat))\n",
        "\n",
        "      tf.summary.scalar('primary_loss', primary_loss, step)\n",
        "      primary_grad = tape.gradient(primary_loss, model.variables)\n",
        "      if mode == 'Single task':       \n",
        "        optimizer.apply_gradients(zip(primary_grad, model.variables))\n",
        "      else:\n",
        "        auxiliary_grad = model.variables #[v*(len(np.array(v.shape))-1) for v in model.variables]\n",
        "        if alpha != 1:\n",
        "          if average_primary_grad is None:\n",
        "            average_primary_grad = [alpha*g for g in primary_grad]\n",
        "          else:\n",
        "            for i in range(len(average_primary_grad)):\n",
        "              if primary_grad[i] is not None:\n",
        "                average_primary_grad[i] = ((1 - alpha)*average_primary_grad[i]\n",
        "                                           + alpha*primary_grad[i])\n",
        "    \n",
        "        censored_auxiliary_grad = combined_grads(primary_grad,\n",
        "                                                 average_primary_grad,\n",
        "                                                 auxiliary_grad,\n",
        "                                                 mode,\n",
        "                                                 overall=overall,\n",
        "                                                 lam=lam)\n",
        "        optimizer.apply_gradients(zip(primary_grad, model.variables))\n",
        "        weights = model.get_weights()\n",
        "        model.set_weights([weights[i] - (optimizer.learning_rate*censored_auxiliary_grad[i]).numpy() for i in range(len(weights))])\n",
        "            \n",
        "  return average_primary_grad, step\n",
        "\n",
        "def get_metrics(dataset,\n",
        "                model,\n",
        "                writer,\n",
        "                step):\n",
        "  with writer.as_default():\n",
        "    if dataset == 'val':\n",
        "      y_hat = model(x_val)\n",
        "      primary_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=y_val, logits=y_hat))\n",
        "    else:\n",
        "      y_hat = model(x_test)\n",
        "      primary_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=y_test, logits=y_hat))\n",
        "    tf.summary.scalar('primary_loss', primary_loss, step)\n",
        "  return primary_loss.numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXBGSc0KmB_6",
        "colab_type": "code",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "def run_experiment(name, model, alpha, mode, overall, lam):\n",
        "  \"\"\"Trains the model until early stopping\n",
        "   \n",
        "  Args:\n",
        "    name: The name to be used for the Tensorboard log files\n",
        "    model: The Keras model being trained\n",
        "    alpha: The factor for the exponential moving average\n",
        "    mode: The method used for adjusting the auxiliary loss gradient:\n",
        "      - Single task: the auxiliary loss gradient is ignored\n",
        "      - Multitask: the auxiliary loss gradient is kept as it is\n",
        "      - Unweighted cosine: cf. https://arxiv.org/abs/1812.02224\n",
        "      - Weighted cosine: cf. https://arxiv.org/abs/1812.02224\n",
        "      - Orthogonal: https://arxiv.org/abs/1801.07593\n",
        "      - Projection: cf. ICML submission\n",
        "    overall: A boolean indicating whether the previous method should be applied to the whole parameter vector\n",
        "    lam: The weight of the auxiliary task\n",
        "    \n",
        "  Returns:\n",
        "    The performance metrics on the test set for the best model on the validation set\n",
        "  \"\"\"\n",
        "  train_writer = tf.summary.create_file_writer('./log/train/' + name,\n",
        "                                               flush_millis=10000)\n",
        "  val_writer = tf.summary.create_file_writer('./log/val/' + name,\n",
        "                                             flush_millis=10000)\n",
        "  test_writer = tf.summary.create_file_writer('./log/test/' + name,\n",
        "                                              flush_millis=10000)\n",
        "  optimizer = tf.keras.optimizers.Adam()\n",
        "  checkpoint_dir = 'checkpoint'\n",
        "  shutil.rmtree(checkpoint_dir, ignore_errors=True)\n",
        "  checkpoint_prefix = os.path.join(checkpoint_dir, 'model.ckpt')\n",
        "  root = tf.train.Checkpoint(optimizer=optimizer, model=model)\n",
        "\n",
        "  average_primary_grad = None\n",
        "  iteration, step, not_better, best_loss = 1, 0, 0, np.Inf\n",
        "  while not_better < 5:\n",
        "    average_primary_grad, step = train_iteration(model,\n",
        "                                                 average_primary_grad,\n",
        "                                                 alpha,\n",
        "                                                 optimizer,\n",
        "                                                 train_writer,\n",
        "                                                 mode,\n",
        "                                                 step,\n",
        "                                                 overall=overall,\n",
        "                                                 lam=lam)\n",
        "    val_loss = get_metrics('val', model, val_writer, step)\n",
        "    if val_loss < best_loss:\n",
        "      not_better, best_loss = 0, val_loss\n",
        "      root.save(file_prefix=checkpoint_prefix)\n",
        "    else:\n",
        "      not_better += 1\n",
        "    iteration += 1\n",
        "\n",
        "  root.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "  return get_metrics('test', model, test_writer, step)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "2EVnMLpxK3Ur",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results_file = results_dir + \"experiment-4.csv\"\n",
        "\n",
        "if not os.path.isfile(results_file):\n",
        "    with open(results_file, 'w') as file:\n",
        "        file.write('run,mode,overall,alpha,lam,loss')\n",
        "\n",
        "i = !cat {results_file.replace(' ', '\\ ')} | wc -l\n",
        "i = int(i[0])\n",
        "\n",
        "parameters = [(run, mode, overall, lam, alpha)\n",
        "              for run in range(50)\n",
        "              for lam in [0, 300, 100, 30, 10, 3, 1]\n",
        "              for overall in [False]\n",
        "              for mode in ['Single task',\n",
        "                           'Multitask',\n",
        "                           'Projection',\n",
        "                           'Weighted cosine',\n",
        "                           'Unweighted cosine',\n",
        "                           'Orthogonal']\n",
        "              for alpha in [0.01, 1]]\n",
        "\n",
        "parameters = [x for x in parameters if ((x[1] != 'Single task') or (x[3] == 0 and x[4] == 1))]\n",
        "parameters = [x for x in parameters if ((x[1] == 'Single task') or (x[3] != 0))]\n",
        "parameters = [x for x in parameters if ((x[1] not in ['Single task', 'Multitask']) or (x[4] == 1))]\n",
        "\n",
        "while i < len(parameters):\n",
        "  run, mode, overall, lam, alpha = parameters[i]\n",
        "  name = '%s-%s-%s-%f-%f' % (run, mode, overall, lam, alpha)\n",
        "  model = bigger_model()\n",
        "  _ = model(x_val[:1, :])\n",
        "  model.load_weights('%s/model_%i.h5' % (models_dir, run))\n",
        "  result = run_experiment(name, model, alpha, mode, overall, lam)\n",
        "  with open(results_file, 'a') as file:\n",
        "    file.write(f'\\n{run},{mode},{overall},{alpha},{lam},{str(result)}')\n",
        "    print(f'{run},{mode},{overall},{alpha},{lam},{str(result)}')\n",
        "  i += 1"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}